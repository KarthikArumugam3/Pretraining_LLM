# Pretraining_LLMs
A demo of how to use Transformer models for pretraining LLMs

I have used a decoder-only transformer model in this repository to pre-train an LLM model.
